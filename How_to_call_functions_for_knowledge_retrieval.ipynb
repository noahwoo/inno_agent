{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to use functions with a knowledge base\n",
    "\n",
    "This notebook builds on the concepts in the [argument generation]('How_to_generate_function_arguments_with_chat_models.ipynb') notebook, by creating an agent with access to a knowledge base and two functions that it can call based on the user requirement.\n",
    "\n",
    "We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\n",
    "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
    "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n",
    "\n",
    "This will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second.\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "This cookbook takes you through the following workflow:\n",
    "\n",
    "- **Search utilities:** Creating the two functions that access arXiv for answers.\n",
    "- **Configure Agent:** Building up the Agent behaviour that will assess the need for a function and, if one is required, call that function and present results back to the agent.\n",
    "- **arXiv conversation:** Put all of this together in live conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e71f33",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: scipy in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from scipy) (1.24.2)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: tenacity in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (8.2.2)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: tiktoken==0.3.3 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from tiktoken==0.3.3) (2.28.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from tiktoken==0.3.3) (2023.3.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (1.26.15)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: termcolor in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (2.2.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: openai in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (0.27.8)\n",
      "Requirement already satisfied: tqdm in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: requests in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting arxiv\n",
      "  Downloading https://mirrors.bfsu.edu.cn/pypi/web/packages/59/cf/79b5c30438b802e37a5e45805053bed5b362cfca6c7503251ce658a34066/arxiv-1.4.7-py3-none-any.whl (12 kB)\n",
      "Collecting feedparser\n",
      "  Downloading https://mirrors.bfsu.edu.cn/pypi/web/packages/92/1e/741fd94cf2855d251712868f2183cb6485a28daaa3947e1a7046dc036aca/feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sgmllib3k\n",
      "  Downloading https://mirrors.bfsu.edu.cn/pypi/web/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=537f3e86e3240363c55a628b94bf475fa59489504d846d3ce0e732fa9772dc0a\n",
      "  Stored in directory: /Users/wujianmin/Library/Caches/pip/wheels/36/4a/12/0f7821c2505886ffcf63d343c94cd26e2005f4267dd9b2b197\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-1.4.7 feedparser-6.0.10 sgmllib3k-1.0.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: pandas in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting PyPDF2\n",
      "  Downloading https://mirrors.bfsu.edu.cn/pypi/web/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/, http://mirrors.aliyun.com/pypi/simple/, http://pypi.douban.com/simple, http://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: tqdm in /Users/wujianmin/miniforge3/envs/py310/lib/python3.10/site-packages (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install tenacity\n",
    "!pip install tiktoken==0.3.3\n",
    "!pip install termcolor \n",
    "!pip install openai\n",
    "!pip install requests\n",
    "!pip install arxiv\n",
    "!pip install pandas\n",
    "!pip install PyPDF2\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dev.chatwithoracle.com/api\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "openai.api_base = \"https://dev.chatwithoracle.com/api/v1\"\n",
    "\n",
    "bare_base = openai.api_base.replace(\"/v1\", \"\")\n",
    "print(bare_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e47962",
   "metadata": {},
   "source": [
    "## Search utilities\n",
    "\n",
    "We'll first set up some utilities that will underpin our two functions.\n",
    "\n",
    "Downloaded papers will be stored in a directory (we use ```./data/papers``` here). We create a file ```arxiv_library.csv``` to store the embeddings and details for downloaded papers to retrieve against using ```summarize_text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2de5d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "doc_root = 'data'\n",
    "\n",
    "data_dir = os.path.join(doc_root, 'papers')\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "57217b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "def build_paper_head(paper_head, prefix = None) :\n",
    "    # f\"Title: {strings[0]['title']}\\nAuthor: {strings[0]['authors']}\\nPublished: {strings[0]['published']}\\n\" \n",
    "    head = ''\n",
    "    if prefix is not None: \n",
    "        head = f\"#{prefix}. \"\n",
    "    \n",
    "    head += f\"**[{paper_head['title']}]({paper_head['article_url']})**\\n- **Author**\"\n",
    "\n",
    "    authors = paper_head['authors']\n",
    "    if isinstance(authors, str) :\n",
    "        authors = eval(authors)\n",
    "    \n",
    "    for author in authors:\n",
    "        head += f\"\\n  - {author.name}\"\n",
    "    head += f\"\\n- **Published**: {paper_head['published']}\"\n",
    "    return head\n",
    "\n",
    "def search_article_list(query, library=paper_dir_filepath, top_k=5, recent_days=None):\n",
    "    \"\"\"\n",
    "    This function gets the top_k articles based on a user's query, sorted by relevance and recency. \n",
    "    Return the title, summary, authors and published date for each article\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "        result_dict.update({\"authors\": result.authors})\n",
    "        result_dict.update({\"published\": result.published})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "    \n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.authors,\n",
    "            result.published,\n",
    "            result_dict['article_url'],\n",
    "            result.download_pdf(data_dir),\n",
    "            response[\"data\"][0][\"embedding\"],\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "            \n",
    "    # compose result manually\n",
    "    if len(result_list) == 0:\n",
    "        return \"No results found\"\n",
    "    result_buffer = f\"Search results for '**{query}**': \\n\\n\"\n",
    "    for idx, result in enumerate(result_list):\n",
    "        result_buffer += build_paper_head(result, idx) + \"\\n\\n\"\n",
    "\n",
    "    return result_buffer\n",
    "\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=1):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=f\"{query}:title\", max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    result_list = []\n",
    "    \n",
    "    nresult = 0\n",
    "    for result in search.results():\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "        result_dict.update({\"authors\": result.authors})\n",
    "        result_dict.update({\"published\": result.published})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.authors,\n",
    "            result.published,\n",
    "            result_dict['article_url'],\n",
    "            result.download_pdf(data_dir),\n",
    "            response[\"data\"][0][\"embedding\"],\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "        nresult += 1\n",
    "\n",
    "    print(f\"Searching for '{query}' with topk={top_k}, get {nresult} results\\n\")\n",
    "    print(f\"Top-ranked paper's title: {result_list[0]['title']}\")\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "dda02bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'Prompt Engineering' with topk=1, get 1 results\n",
      "\n",
      "Top-ranked paper's title: Prompt Agnostic Essay Scorer: A Domain Generalization Approach to Cross-prompt Automated Essay Scoring\n"
     ]
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "# result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output = get_articles(\"Prompt Engineering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e710591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:Yi-Lin Tuan\n",
      "------------------\n",
      "name:Jinzhi Zhang\n",
      "------------------\n",
      "name:Yujia Li\n",
      "------------------\n",
      "name:Hung-yi Lee\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# details about the author\n",
    "for author in result_output[0]['authors'] :\n",
    "    # print(f\"name:{author.name}, affiliation:{author.arxiv.affiliation}, email:{author.email}\")\n",
    "    print(f\"name:{author.name}\")\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "11b88153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([3, 4], 0.4)\n",
      "Yi-Lin Tuan\n",
      "Jinzhi Zhang\n",
      "Yujia Li\n",
      "Hung-yi Lee\n"
     ]
    }
   ],
   "source": [
    "a = ([3,4], 0.4)\n",
    "print(a)\n",
    "\n",
    "authors = \"[arxiv.Result.Author('Yi-Lin Tuan'), arxiv.Result.Author('Jinzhi Zhang'), arxiv.Result.Author('Yujia Li'), arxiv.Result.Author('Hung-yi Lee')]\"\n",
    "\n",
    "for author in eval(authors):\n",
    "    print(author.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "11675627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    "    rel_th : float = 0.95\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        ({\"filepath\" : row[\"filepath\"], \"title\": row[\"title\"], \n",
    "          \"authors\" : row[\"authors\"], \"published\" : row[\"published\"], \n",
    "          \"article_url\" : row[\"article_url\"]}, \n",
    "         relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    if relatednesses[top_n-1] > rel_th :\n",
    "        print(f\"Find paper with relatedness {relatednesses[top_n-1]} > {rel_th}!\\n\")\n",
    "        return strings[:top_n]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7211df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarize chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        result_list = get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "\n",
    "    # rank by relatedness\n",
    "    library_df.columns = [\"title\", \"authors\", \"published\", \"article_url\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1, rel_th=0.95)\n",
    "\n",
    "    if strings is None : # no-related papers in local repository, search arXiv again\n",
    "        result_list = get_articles(query)\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "        library_df.columns = [\"title\", \"authors\", \"published\", \"article_url\", \"filepath\", \"embedding\"]\n",
    "        library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "        strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    \n",
    "    assert strings is not None, \"No papers found\"\n",
    "    \n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0]['filepath'])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    # attach title, author & published\n",
    "    paper_head = build_paper_head(strings[0])\n",
    "    response[\"choices\"][0][\"message\"][\"content\"] = paper_head + \"\\n\\n\" + response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "898b94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n",
      "{'filepath': 'data/papers/1808.07982v1.Proximal_Policy_Optimization_and_its_Dynamic_Version_for_Sequence_Generation.pdf', 'title': 'Proximal Policy Optimization and its Dynamic Version for Sequence Generation', 'authors': \"[arxiv.Result.Author('Yi-Lin Tuan'), arxiv.Result.Author('Jinzhi Zhang'), arxiv.Result.Author('Yujia Li'), arxiv.Result.Author('Hung-yi Lee')]\", 'published': '2018-08-24 02:14:43+00:00'}\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c715f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Title**: Proximal Policy Optimization and its Dynamic Version for Sequence Generation\n",
      "- **Author**\n",
      "  - Yi-Lin Tuan\n",
      "  - Jinzhi Zhang\n",
      "  - Yujia Li\n",
      "  - Hung-yi Lee\n",
      "- **Published**: 2018-08-24 02:14:43+00:00\n",
      "\n",
      "Core Argument:\n",
      "- The paper discusses the use of Proximal Policy Optimization (PPO) in sequence generation tasks, specifically in the context of chit-chat chatbots.\n",
      "- The authors argue that PPO is a more efficient reinforcement learning algorithm compared to policy gradient, commonly used in text generation tasks.\n",
      "- They propose a dynamic approach for PPO (PPO-dynamic) and demonstrate its efficacy in synthetic experiments and chit-chat chatbot tasks.\n",
      "\n",
      "Evidence:\n",
      "- PPO-dynamic achieves high precision scores comparable to other algorithms in a synthetic counting task.\n",
      "- PPO-dynamic shows faster progress and a more stable learning curve compared to PPO in both synthetic and chit-chat chatbot tasks.\n",
      "- In the chit-chat chatbot task, PPO-dynamic achieves a slightly higher BLEU-2 score than other algorithms.\n",
      "\n",
      "Conclusions:\n",
      "- PPO is a better optimization method for sequence learning compared to policy gradient.\n",
      "- PPO-dynamic further improves the optimization process by dynamically adjusting hyperparameters.\n",
      "- PPO can be used as a new optimization method for GAN-based sequence learning for better performance.\n"
     ]
    }
   ],
   "source": [
    "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab07e98",
   "metadata": {},
   "source": [
    "## Configure Agent\n",
    "\n",
    "We'll create our agent in this step, including a ```Conversation``` class to support multiple turns with the API, and some Python functions to enable interaction between the ```ChatCompletion``` API and our knowledge base functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "77a6fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    try:\n",
    "        # base = openai.api_base or \"https://api.openai.com\"\n",
    "        base = \"https://api.openai.com\"\n",
    "        base = base.replace(\"v1/\", \"\")\n",
    "        response = requests.post(\n",
    "            f\"{base}/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "73f7672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "978b7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our get_articles and read_article_and_summarize functions\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to download an academic paper from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        }\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"search_and_return_articles\", \n",
    "        \"description\": \"\"\"Use this function to search arXiv database for a list of articles related to user query and return, \n",
    "        argument 'top_k' should be an integer for the number of articles to return,\n",
    "        argument 'recency_days' should be an integer for the number of days since the article was published\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"Subject of the article in plain text\"\"\"\n",
    "                },\n",
    "                \"top_k\" : {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": f\"\"\"Number of articles to return\"\"\"\n",
    "                }, \n",
    "                \"recent_days\" : {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": f\"\"\"Number of days since the article was published\"\"\"\n",
    "                }\n",
    "            }, \n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before search_and_return_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0c88ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.json()[\"choices\"][0]\n",
    "    if full_message[\"finish_reason\"] == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function {full_message}\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "    func_name = full_message[\"message\"][\"function_call\"][\"name\"]\n",
    "    print(f'Get function name: {func_name}')\n",
    "    if func_name == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(\n",
    "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "            )\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif func_name == \"read_article_and_summarize\":\n",
    "        parsed_output = json.loads(\n",
    "            full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "    elif func_name == \"search_and_return_articles\" :\n",
    "        arguments = eval(full_message[\"message\"][\"function_call\"][\"arguments\"])\n",
    "        article_lst = search_article_list(arguments['query'], top_k=arguments['top_k'])\n",
    "        return article_lst\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3e7868",
   "metadata": {},
   "source": [
    "## arXiv conversation\n",
    "\n",
    "Let's put this all together by testing our functions out in conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c39a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "# paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "# You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "# You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "# Begin!\"\"\"\n",
    "\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions. \n",
    "You search for a list of papers to answer user questions. You read and summarize the paper clearly according to user provided topics.\n",
    "Please summarize in clear and concise format. Begin!\"\"\"\n",
    "\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "253fd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function {'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'search_and_return_articles', 'arguments': '{\\n  \"query\": \"prompt engineering\",\\n  \"top_k\": 5,\\n  \"recent_days\": 365\\n}'}}, 'finish_reason': 'function_call'}\n",
      "Get function name: search_and_return_articles\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Search results for '**prompt engineering**': \n",
       "\n",
       "#0. **[Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering](http://arxiv.org/abs/2303.13534v1)**\n",
       "- **Author**\n",
       "  - Jonas Oppenlaender\n",
       "  - Rhema Linder\n",
       "  - Johanna Silvennoinen\n",
       "- **Published**: 2023-03-13 14:25:56+00:00\n",
       "\n",
       "#1. **[Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models](http://arxiv.org/abs/2306.03799v1)**\n",
       "- **Author**\n",
       "  - Fobo Shi\n",
       "  - Peijun Qing\n",
       "  - Dong Yang\n",
       "  - Nan Wang\n",
       "  - Youbo Lei\n",
       "  - Haonan Lu\n",
       "  - Xiaodong Lin\n",
       "- **Published**: 2023-06-06 15:43:16+00:00\n",
       "\n",
       "#2. **[Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](http://arxiv.org/abs/2305.13860v1)**\n",
       "- **Author**\n",
       "  - Yi Liu\n",
       "  - Gelei Deng\n",
       "  - Zhengzi Xu\n",
       "  - Yuekang Li\n",
       "  - Yaowen Zheng\n",
       "  - Ying Zhang\n",
       "  - Lida Zhao\n",
       "  - Tianwei Zhang\n",
       "  - Yang Liu\n",
       "- **Published**: 2023-05-23 09:33:38+00:00\n",
       "\n",
       "#3. **[A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](http://arxiv.org/abs/2302.11382v1)**\n",
       "- **Author**\n",
       "  - Jules White\n",
       "  - Quchen Fu\n",
       "  - Sam Hays\n",
       "  - Michael Sandborn\n",
       "  - Carlos Olea\n",
       "  - Henry Gilbert\n",
       "  - Ashraf Elnashar\n",
       "  - Jesse Spencer-Smith\n",
       "  - Douglas C. Schmidt\n",
       "- **Published**: 2023-02-21 12:42:44+00:00\n",
       "\n",
       "#4. **[A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models](http://arxiv.org/abs/2302.06235v1)**\n",
       "- **Author**\n",
       "  - James Urquhart Allingham\n",
       "  - Jie Ren\n",
       "  - Michael W Dusenberry\n",
       "  - Jeremiah Zhe Liu\n",
       "  - Xiuye Gu\n",
       "  - Yin Cui\n",
       "  - Dustin Tran\n",
       "  - Balaji Lakshminarayanan\n",
       "- **Published**: 2023-02-13 10:19:58+00:00\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, what is the most recent 5 papers on prompt enginnering?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "\n",
    "if isinstance(chat_response, str):\n",
    "    assistant_message = chat_response\n",
    "else :\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3ca3e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function {'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'read_article_and_summarize', 'arguments': '{\\n  \"query\": \"Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering\"\\n}'}}, 'finish_reason': 'function_call'}\n",
      "Get function name: read_article_and_summarize\n",
      "Finding and reading paper\n",
      "Find paper with relatedness 1 > 0.95!\n",
      "\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:15<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**[Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering](http://arxiv.org/abs/2303.13534v1)**\n",
       "- **Author**\n",
       "  - Jonas Oppenlaender\n",
       "  - Rhema Linder\n",
       "  - Johanna Silvennoinen\n",
       "- **Published**: 2023-03-13 14:25:56+00:00\n",
       "\n",
       "Core Argument:\n",
       "- This academic paper investigates the creative skill of prompt engineering in the context of AI art.\n",
       "- Prompt engineering involves iteratively crafting prompts to generate and improve digital images using AI.\n",
       "- The paper presents three studies conducted with participants recruited from a crowdsourcing platform to explore their ability to recognize the quality of prompts, write prompts, and improve their prompts.\n",
       "- Prompt engineering is a learned skill that requires expertise and practice.\n",
       "\n",
       "Evidence:\n",
       "- Participants were able to assess the quality of prompts and images, with this ability increasing with their experience and interest in art.\n",
       "- Participants were able to write prompts in rich descriptive language, but lacked the specific vocabulary needed to apply a certain style to the generated images.\n",
       "- Participants' familiarity with art was negatively correlated with the average error in consistency, suggesting that participants with more art experience were better at predicting the quality of the generated artworks.\n",
       "- Participants were able to generate effective input prompts for text-to-image systems, demonstrating an understanding of how to maximize the visual attractiveness and aesthetic qualities of the generated artworks.\n",
       "- Participants made changes to their prompts by adding more tokens, particularly nouns and adjectives.\n",
       "- Over half of the revised sets of images showed no improvement in image quality, while a third were better and 15% were worse.\n",
       "- Participants were able to make improvements by adding more details and improving colors and contrast.\n",
       "- Changes in the style of the images were rare, with most revised sets being in the same or similar style.\n",
       "\n",
       "Conclusions:\n",
       "- Prompt engineering is a skill that requires expertise and knowledge of certain keywords and phrases.\n",
       "- Prompt engineering could become an expert skill limited to a small group of professionals or a common practice used in everyday life.\n",
       "- Prompt engineering may become obsolete as generative systems improve their ability to understand user intent.\n",
       "- Prompt engineering could become a personal signature or curation skill in the future.\n",
       "- Recommendations for conducting text-to-image experiments with crowd workers include providing clear instructions, explaining keywords and key phrases, and considering the limitations of crowd workers.\n",
       "- The findings offer a deeper understanding of prompt engineering and open up avenues for future research in this field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Can you read the paper of Prompt AI art, and give me a summary?\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "\n",
    "if isinstance(updated_response, str):\n",
    "    assistant_message = updated_response\n",
    "else :\n",
    "    assistant_message = updated_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7057a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function {'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'read_article_and_summarize', 'arguments': '{\\n  \"query\": \"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study\"\\n}'}}, 'finish_reason': 'function_call'}\n",
      "Get function name: read_article_and_summarize\n",
      "Finding and reading paper\n",
      "Find paper with relatedness 1 > 0.95!\n",
      "\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:15<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**[Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](http://arxiv.org/abs/2305.13860v1)**\n",
       "- **Author**\n",
       "  - Yi Liu\n",
       "  - Gelei Deng\n",
       "  - Zhengzi Xu\n",
       "  - Yuekang Li\n",
       "  - Yaowen Zheng\n",
       "  - Ying Zhang\n",
       "  - Lida Zhao\n",
       "  - Tianwei Zhang\n",
       "  - Yang Liu\n",
       "- **Published**: 2023-05-23 09:33:38+00:00\n",
       "\n",
       "Core Argument:\n",
       "- The academic paper investigates the concept of \"jailbreaking\" Large Language Models (LLMs), specifically focusing on the C HATGPT model.\n",
       "- The study aims to answer three research questions: the number of different prompt types that can jailbreak LLMs, the effectiveness of jailbreak prompts in bypassing LLM constraints, and the resilience of C HATGPT against these jailbreak prompts.\n",
       "\n",
       "Evidence:\n",
       "- The paper discusses the popularity and adoption of LLMs like C HATGPT and the limitations imposed by OpenAI to address concerns of misuse.\n",
       "- The study collects 78 verified jailbreak prompts and develops a classification model to analyze their distribution.\n",
       "- The jailbreak prompts are tested on C HATGPT versions 3.5 and 4.0 using a dataset of 3,120 jailbreak questions across eight prohibited scenarios.\n",
       "- The paper evaluates the resistance of C HATGPT against jailbreak prompts and identifies factors that affect their capabilities.\n",
       "- The paper compares the protection power of GPT-3.5-TURBO and GPT-4 and evaluates the strength of jailbreak prompts.\n",
       "\n",
       "Conclusions:\n",
       "- The study provides insights into the types and capabilities of jailbreak prompts, the effectiveness of these prompts in bypassing LLM restrictions, and the resilience of C HATGPT against jailbreak attempts.\n",
       "- The paper highlights the importance of prompt structures in jailbreaking LLMs and discusses the challenges of generating robust jailbreak prompts and preventing prompt-based jailbreaks.\n",
       "- The paper emphasizes the need to align content policy strength with real-world laws and ethical standards.\n",
       "- The study suggests possible future research directions to enhance the robustness and security of language models like ChatGPT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"OK, give me a summary of the paper about Jailbreaking ChatGPT.\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "\n",
    "if isinstance(updated_response, str):\n",
    "    assistant_message = updated_response\n",
    "else :\n",
    "    assistant_message = updated_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(assistant_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9df96fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function {'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'search_and_return_articles', 'arguments': '{\\n  \"query\": \"parameter efficient tuning\",\\n  \"top_k\": 5\\n}'}}, 'finish_reason': 'function_call'}\n",
      "Get function name: search_and_return_articles\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Search results for '**parameter efficient tuning**': \n",
       "\n",
       "#0. **[Parameter-Efficient Fine-Tuning Design Spaces](http://arxiv.org/abs/2301.01821v1)**\n",
       "- **Author**\n",
       "  - Jiaao Chen\n",
       "  - Aston Zhang\n",
       "  - Xingjian Shi\n",
       "  - Mu Li\n",
       "  - Alex Smola\n",
       "  - Diyi Yang\n",
       "- **Published**: 2023-01-04 21:00:18+00:00\n",
       "\n",
       "#1. **[Parameter-Efficient Tuning on Layer Normalization for Pre-trained Language Models](http://arxiv.org/abs/2211.08682v3)**\n",
       "- **Author**\n",
       "  - Wang Qi\n",
       "  - Yu-Ping Ruan\n",
       "  - Yuan Zuo\n",
       "  - Taihao Li\n",
       "- **Published**: 2022-11-16 05:31:49+00:00\n",
       "\n",
       "#2. **[Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization](http://arxiv.org/abs/2305.03937v1)**\n",
       "- **Author**\n",
       "  - Anastasia Razdaibiedina\n",
       "  - Yuning Mao\n",
       "  - Rui Hou\n",
       "  - Madian Khabsa\n",
       "  - Mike Lewis\n",
       "  - Jimmy Ba\n",
       "  - Amjad Almahairi\n",
       "- **Published**: 2023-05-06 05:35:14+00:00\n",
       "\n",
       "#3. **[Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning](http://arxiv.org/abs/2210.08823v3)**\n",
       "- **Author**\n",
       "  - Dongze Lian\n",
       "  - Daquan Zhou\n",
       "  - Jiashi Feng\n",
       "  - Xinchao Wang\n",
       "- **Published**: 2022-10-17 08:14:49+00:00\n",
       "\n",
       "#4. **[Scattered or Connected? An Optimized Parameter-efficient Tuning Approach for Information Retrieval](http://dx.doi.org/10.1145/3511808.3557445)**\n",
       "- **Author**\n",
       "  - Xinyu Ma\n",
       "  - Jiafeng Guo\n",
       "  - Ruqing Zhang\n",
       "  - Yixing Fan\n",
       "  - Xueqi Cheng\n",
       "- **Published**: 2022-08-21 08:56:02+00:00\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Give me recent 5 papers about parameter efficient tuning?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "\n",
    "if isinstance(chat_response, str):\n",
    "    assistant_message = chat_response\n",
    "else :\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "057718d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function {'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'read_article_and_summarize', 'arguments': '{\\n  \"query\": \"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization\"\\n}'}}, 'finish_reason': 'function_call'}\n",
      "Get function name: read_article_and_summarize\n",
      "Finding and reading paper\n",
      "Find paper with relatedness 1 > 0.95!\n",
      "\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:12<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**[Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization](http://arxiv.org/abs/2305.03937v1)**\n",
       "- **Author**\n",
       "  - Anastasia Razdaibiedina\n",
       "  - Yuning Mao\n",
       "  - Rui Hou\n",
       "  - Madian Khabsa\n",
       "  - Mike Lewis\n",
       "  - Jimmy Ba\n",
       "  - Amjad Almahairi\n",
       "- **Published**: 2023-05-06 05:35:14+00:00\n",
       "\n",
       "Core Argument:\n",
       "- The academic paper introduces a method called Residual Prompt Tuning that improves the performance and stability of prompt tuning for parameter-efficient tuning of pretrained language models.\n",
       "\n",
       "Evidence:\n",
       "- Residual Prompt Tuning reparameterizes soft prompt embeddings using a shallow network with a residual connection, which significantly outperforms prompt tuning on the SuperGLUE benchmark.\n",
       "- The method allows for a reduction in prompt length without sacrificing performance and is robust to different learning rates and prompt initializations.\n",
       "- The experiments are conducted on NLU tasks from the SuperGLUE benchmark using BERT and T5 models, comparing Residual Prompt Tuning against other prompt reparameterization techniques and parameter-efficient tuning methods.\n",
       "- Residual Prompt Tuning outperforms other reparameterization techniques and achieves competitive performance compared to fine-tuning.\n",
       "- The use of residual connections and parameter sharing in the reparameterization network improves performance and offers parameter efficiency and knowledge sharing benefits.\n",
       "\n",
       "Conclusions:\n",
       "- Residual Prompt Tuning is an effective method for improving the performance of transfer learning-based models in natural language processing tasks.\n",
       "- Residual Prompt Tuning is robust to hyperparameter choice, speeds up convergence, and is effective in few-shot settings.\n",
       "- The method requires the same number of additional parameters during inference as the original prompt tuning method, but it requires 25 times fewer extra parameters compared to adapter-based methods.\n",
       "- The paper acknowledges the contributions of other researchers in the field and discusses the ethical implications of the proposed method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Thanks, please summary the paper about Residual Prompt Tuning.\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "\n",
    "if isinstance(updated_response, str):\n",
    "    assistant_message = updated_response\n",
    "else :\n",
    "    assistant_message = updated_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(assistant_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "291df080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function {'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'read_article_and_summarize', 'arguments': '{\\n  \"query\": \"Fine-Tuning Design Spaces\"\\n}'}}, 'finish_reason': 'function_call'}\n",
      "Get function name: read_article_and_summarize\n",
      "Finding and reading paper\n",
      "Find paper with relatedness 0.96118840857324 > 0.95!\n",
      "\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:15<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**[Parameter-Efficient Fine-Tuning Design Spaces](http://arxiv.org/abs/2301.01821v1)**\n",
       "- **Author**\n",
       "  - Jiaao Chen\n",
       "  - Aston Zhang\n",
       "  - Xingjian Shi\n",
       "  - Mu Li\n",
       "  - Alex Smola\n",
       "  - Diyi Yang\n",
       "- **Published**: 2023-01-04 21:00:18+00:00\n",
       "\n",
       "The academic paper discusses parameter-efficient fine-tuning in natural language processing tasks. The authors propose a parameter-efficient fine-tuning design paradigm and discover design patterns that can be applied to different experimental settings. They introduce parameter-efficient fine-tuning design spaces that parameterize tuning structures and strategies. The design spaces are characterized by layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. The authors progressively refine the design space based on the model quality of each design choice and make greedy selections. They discover design patterns such as grouping layers in a spindle pattern, allocating trainable parameters uniformly, tuning all groups, and assigning proper tuning strategies to different groups. These design patterns result in new parameter-efficient fine-tuning methods that consistently outperform existing strategies. The authors also show that these methods are applicable to different backbone models and tasks in natural language processing. The contributions of the paper include introducing parameter-efficient fine-tuning design spaces, discovering design patterns through experiments, and developing parameter-efficient fine-tuning methods that outperform existing strategies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Thanks, please summary the paper about Fine-Tuning Design Spaces.\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "\n",
    "if isinstance(updated_response, str):\n",
    "    assistant_message = updated_response\n",
    "else :\n",
    "    assistant_message = updated_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(assistant_message))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
